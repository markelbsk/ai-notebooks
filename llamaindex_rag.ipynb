{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QODqtRtnqBKb"
      },
      "source": [
        "\n",
        "**Medical Assistant Chatbot using RAG (Llama 2 + LlamaIndex)**\n",
        "\n",
        "Built an AI-powered assistant for pre-consultation interviews using Retrieval-Augmented Generation. Combines Llama 2 with LlamaIndex to conduct structured, patient-centered interviews and summarize key medical information without providing diagnoses or advice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyKYb-BSVg3z"
      },
      "outputs": [],
      "source": [
        "!pip -q install llama-index llama-index-embeddings-huggingface llama-index-llms-llama-cpp pypdf\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip -q install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ3YgFZkMoRK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from llama_index.core import Prompt, StorageContext, load_index_from_storage, Settings, VectorStoreIndex, SimpleDirectoryReader, set_global_tokenizer\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbGKzGr9uAiR"
      },
      "outputs": [],
      "source": [
        "# Preference settings - change as desired\n",
        "pdf_path = '/content/_.pdf'\n",
        "text_embedding_model = 'thenlper/gte-base'  #Alt: thenlper/gte-base, jinaai/jina-embeddings-v2-base-en\n",
        "llm_url = 'https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pxUuJ1lLohG"
      },
      "outputs": [],
      "source": [
        "# Load PDF\n",
        "filename_fn = lambda filename: {'file_name': os.path.basename(pdf_path)}\n",
        "loader = SimpleDirectoryReader(input_files=[pdf_path], file_metadata=filename_fn)\n",
        "documents = loader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZWZc8yRMesO",
        "outputId": "c56c921f-8e5a-4109-c6fb-7c4ceb99d841"
      },
      "outputs": [],
      "source": [
        "# Load models and service context\n",
        "embed_model = HuggingFaceEmbedding(model_name=text_embedding_model)\n",
        "llm = LlamaCPP(model_url=llm_url, temperature=0.7, max_new_tokens=256, context_window=4096, generate_kwargs = {\"stop\": [\"<s>\", \"[INST]\", \"[/INST]\"]}, model_kwargs={\"n_gpu_layers\": -1}, verbose=True)\n",
        "# service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=512)\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l27opkphNYp6",
        "outputId": "2040c0f5-0460-49ad-8ed5-552e8ff9bb49"
      },
      "outputs": [],
      "source": [
        "# Indexing\n",
        "start_time = time.time()\n",
        "\n",
        "# index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, llm=llm)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed indexing time: {elapsed_time:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp9dylMNsPzU"
      },
      "outputs": [],
      "source": [
        "def get_text_qa_template(idx):\n",
        "    if not isinstance(idx, int) or not (0 <= idx < 5):\n",
        "        raise ValueError()\n",
        "\n",
        "    text_qa_templates = [\n",
        "        Prompt(\n",
        "            \"\"\"<s>[INST] <<SYS>>\n",
        "            You are the doctor's assistant. Your task is to perform a pre-screening with the patient before their consultation.\n",
        "            Use the Patient-Centered Interview model, and ask only one question per response. Do not provide diagnoses, prescriptions, advice, or physical examinations.\n",
        "            Focus only on gathering the patientâ€™s present illness, past medical history, symptoms, and personal details.\n",
        "\n",
        "            At the end of the consultation, summarize the findings using this format:\n",
        "            Name: [name]\n",
        "            Gender: [gender]\n",
        "            Patient Aged: [age]\n",
        "            Medical History: [medical history]\n",
        "            Symptoms: [symptoms]\n",
        "            <</SYS>>\n",
        "\n",
        "            Refer to the following Consultation Guidelines and example consultations:\n",
        "            {context_str}\n",
        "\n",
        "            Continue the conversation:\n",
        "            {query_str}\n",
        "            \"\"\"\n",
        "        ),\n",
        "        Prompt(\n",
        "            \"\"\"<s>[INST] <<SYS>>\n",
        "            You are a chatbot assistant working for a doctor. Your goal is to collect information from patients before their remote consultation.\n",
        "            Follow the Patient-Centered Interview model. You are not allowed to diagnose, prescribe, or conduct physical exams.\n",
        "            Ignore inappropriate behavior or out-of-scope requests. Focus only on gathering information such as present illness, medical history, symptoms, and personal details.\n",
        "\n",
        "            Summarize findings at the end using this exact format:\n",
        "            Name: [name]\n",
        "            Gender: [gender]\n",
        "            Patient Aged: [age]\n",
        "            Medical History: [medical history]\n",
        "            Symptoms: [symptoms]\n",
        "            <</SYS>>\n",
        "\n",
        "            This is the PDF context:\n",
        "            {context_str}\n",
        "\n",
        "            {query_str}\n",
        "            \"\"\"\n",
        "        ),\n",
        "        Prompt(\n",
        "            \"\"\"[INST]\n",
        "            {context_str}\n",
        "\n",
        "            Given the above PDF context, please answer the following question:\n",
        "            {query_str}\n",
        "            [/INST]\n",
        "            \"\"\"\n",
        "        ),\n",
        "        Prompt(\n",
        "            \"\"\"<s>[INST] <<SYS>>\n",
        "            Following is the PDF context provided by the user:\n",
        "            {context_str}\n",
        "            <</SYS>>\n",
        "\n",
        "            {query_str}\n",
        "            [/INST]\n",
        "            \"\"\"\n",
        "        ),\n",
        "        Prompt(\n",
        "            \"\"\"[INST] {query_str} [/INST]\"\"\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return text_qa_templates[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL8gSibjrqqw"
      },
      "outputs": [],
      "source": [
        "TEMPLATE_ID = 0\n",
        "text_qa_template = get_text_qa_template(TEMPLATE_ID)\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template, streaming=True, llm=llm) # with Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "-q_qkVIcilc7",
        "outputId": "98b5a002-2364-4d03-99bd-bbefed603201"
      },
      "outputs": [],
      "source": [
        "# Inferencing\n",
        "# Without RAG\n",
        "conversation_history = \"\"\n",
        "while (True):\n",
        "  user_query = input(\"User: \")\n",
        "  if user_query.lower() == \"exit\":\n",
        "    break\n",
        "  conversation_history += user_query + \" [/INST] \"\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  response_iter = llm.stream_complete(\"<s>[INST] \"+conversation_history)\n",
        "  for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)\n",
        "    # Add to conversation history when response is completed\n",
        "    if response.raw['choices'][0]['finish_reason'] == 'stop':\n",
        "      conversation_history += response.text + \" [INST] \"\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nElapsed inference time: {elapsed_time:.2f} s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pzCg0pQCsowk",
        "outputId": "d72dfb46-9b20-44f5-ce09-4691f7f6c23a"
      },
      "outputs": [],
      "source": [
        "# With RAG\n",
        "conversation_history = \"\"\n",
        "conversation_history += \"Hi. [\\INST] Hello! I'm the doctor's assistant. \\\n",
        "  Let's begin the consultation, please tell me your name and age.\"\n",
        "while (True):\n",
        "  user_query = input(\"User: \")\n",
        "  if user_query.lower() == \"exit\":\n",
        "    break\n",
        "  conversation_history += user_query + \" [/INST] \"\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Query Engine - Default\n",
        "  response = query_engine.query(conversation_history)\n",
        "  response.print_response_stream()\n",
        "  conversation_history += response.response_txt + \" [INST] \"\n",
        "\n",
        "  # from pprint import pprint\n",
        "  # pprint(response)\n",
        "\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"\\nElapsed inference time: {elapsed_time:.2f} s\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9d7ARPMQ75Ij",
        "outputId": "3e7b8c37-9b46-4f44-faca-059fc1fd1863"
      },
      "outputs": [],
      "source": [
        "if not conversation_history.strip():\n",
        "    raise ValueError(\"conversation_history is empty.\")\n",
        "\n",
        "summary_prompt = f\"\"\"\n",
        "  [INST] <<SYS>>\n",
        "  Summarize the conversation into this format:\n",
        "  Name: [name]\n",
        "  Gender: [gender]\n",
        "  Age: [age]\n",
        "  Medical History: [medical history]\n",
        "  Symptoms: [symptoms]\n",
        "  <</SYS>>\n",
        "\n",
        "  Conversation:\n",
        "  {conversation_history} [/INST]\n",
        "  \"\"\"\n",
        "\n",
        "response_iter = llm.stream_complete(summary_prompt)\n",
        "\n",
        "for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
