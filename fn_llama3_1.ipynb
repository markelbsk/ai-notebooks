{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTaDCGTe78bK"
      },
      "source": [
        "Fine-tune Llama 3.1 8B with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoPKQjga6obN",
        "outputId": "8a0acbf4-2862-4763-9d18-8de7b5c24ead"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install -qqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off\n",
        "\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "matKaF-f-GiU"
      },
      "source": [
        "## 1. Load model for PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrKqkpbSeSWA"
      },
      "outputs": [],
      "source": [
        "# SFT involves retraining base models on a smaller dataset of instructions and answers.\n",
        "# 3 most popular SFT techniques are full ft, LoRa and QLoRa.\n",
        "#   1. Full ft. It involves retraining all parameters of a pre-trained model on an instruction dataset\n",
        "#   2. LoRa. Instead of retraining the entire model, it freezes the weights and introduces small adapters (low-rank matrices) at each targeted layer.\n",
        "#   3. QLoRa. is an extension of LoRA that offers even greater memory savings (%33 additional memory reduction). We will use this because we are using Google Colab, which has some limitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGX9wG7Lhc-z",
        "outputId": "83bec6a3-4d0d-4015-b3b6-e66d49174e48"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "# Llama 3.1 4bit version (smaller and faster compared to 16 bit version)\n",
        "# 2048 context length. Llama supports up to 128k tokens.\n",
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "# Prepare model for PEFT\n",
        "# Rank (r): matrix size. 16x16\n",
        "# Alpha (lora_alpha): scaling factor for updates. Impacts directly the adapters contribution.\n",
        "# Target modules: model components to retrain.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True, # modifies the scaling factor of LoRA adapters to be proportional to 1/√r instead of 1/r.\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")\n",
        "print(model.print_trainable_parameters())\n",
        "# trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
        "# we’ll only train 42 million out of 8 billion parameters (0.5196%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjDpwfjJ3RAL"
      },
      "source": [
        "## 2. Prepare data and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqGnvaT8is-R",
        "outputId": "44a49de8-c96c-453e-a6c1-69fe660e9caf"
      },
      "outputs": [],
      "source": [
        "# chatml (openai). adds two special tokens (<|im_start|> and <|im_end|>) to indicate who is speaking.\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"chatml\",\n",
        "    mapping={\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}\n",
        ")\n",
        "\n",
        "def apply_template(examples):\n",
        "    messages = examples[\"conversations\"]\n",
        "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:10000]\") # only 10000 samples because our limitations with GPUs (google colab.. # only 10000 samples for bnecaecuaasuuse ofur limitaiton tions with GPUTs (google colab...)\n",
        "# we apply chat template to every conversation\n",
        "dataset = dataset.map(apply_template, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdfjufQd3XMi"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZtd6gmUcGWr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "ArocegjGZ2Hc",
        "outputId": "84512b94-9b4b-4bab-a2f7-eb3f6834f68e"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "config = SFTConfig(\n",
        "    output_dir=\"output\",\n",
        "    learning_rate=3e-4,\n",
        "    lr_scheduler_type=\"linear\", # adjust learning rate\n",
        "    per_device_train_batch_size=2, # only 2 because of efficiency\n",
        "    gradient_accumulation_steps=2,\n",
        "    # per_device_train_batch_size=4,\n",
        "    # gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    # fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\", # 8bit for memory savings\n",
        "    report_to=[],\n",
        "    weight_decay=0.01, # regularizer\n",
        "    warmup_steps=10,\n",
        "    seed=0,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=True,\n",
        "    dataset_num_proc=2,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=config,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI_U9FHZ3ZLO"
      },
      "source": [
        "## 4. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JXdjsLqkvZY"
      },
      "outputs": [],
      "source": [
        "# Load model for inference\n",
        "# simple test not used for evaluation\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
